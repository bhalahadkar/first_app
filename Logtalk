

import openai
import faiss
import numpy as np

# ğŸ” Set your OpenAI API key
openai.api_key = "your-openai-api-key"

# ğŸ“¥ Load and parse FIX file
def load_fix_file(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        raw = f.read()
    # FIX delimiter is typically ASCII 0x01
    return raw.split('\x01') if '\x01' in raw else raw.split('|')

# ğŸ§± Chunk messages into sections for embedding
def chunk_messages(messages, chunk_size=20):
    chunks = []
    for i in range(0, len(messages), chunk_size):
        chunk = messages[i:i + chunk_size]
        chunk_text = " | ".join(chunk).strip()
        if chunk_text:
            chunks.append(chunk_text)
    return chunks

# ğŸ§  Get OpenAI Embeddings
def embed_texts(texts, model="text-embedding-3-small"):
    embeddings = []
    batch_size = 100
    for i in range(0, len(texts), batch_size):
        response = openai.Embedding.create(
            input=texts[i:i+batch_size],
            model=model
        )
        batch_embeddings = [d['embedding'] for d in response['data']]
        embeddings.extend(batch_embeddings)
    return embeddings

# ğŸ§² Create FAISS vector index
def create_faiss_index(vectors):
    dim = len(vectors[0])
    index = faiss.IndexFlatL2(dim)
    index.add(np.array(vectors).astype('float32'))
    return index

# ğŸ” Search and generate GPT answer
def search_and_ask(query, chunks, index, model="gpt-4", top_k=3):
    q_embedding = embed_texts([query])[0]
    D, I = index.search(np.array([q_embedding]).astype('float32'), top_k)
    context = "\n".join([chunks[i] for i in I[0]])

    prompt = f"""You are a FIX protocol assistant. Based on the FIX messages below, answer the user question.

FIX Messages:
{context}

Question: {query}
Answer:"""

    response = openai.ChatCompletion.create(
        model=model,
        messages=[{"role": "user", "content": prompt}],
        temperature=0.2
    )
    return response['choices'][0]['message']['content']

# ğŸ§ª Run full flow
if __name__ == "__main__":
    fix_file_path = "your_fix_file.txt"  # Replace with your 10MB FIX file path

    print("Parsing FIX file...")
    fix_messages = load_fix_file(fix_file_path)

    print("Chunking messages...")
    fix_chunks = chunk_messages(fix_messages, chunk_size=25)

    print(f"Total chunks: {len(fix_chunks)}")

    print("Generating embeddings...")
    fix_embeddings = embed_texts(fix_chunks)

    print("Building vector index...")
    index = create_faiss_index(fix_embeddings)

    # ğŸ§  Example question
    user_query = input("\nAsk a question about the FIX file: ")

    print("\nSearching and generating answer...\n")
    answer = search_and_ask(user_query, fix_chunks, index)
    print("ğŸ“˜ Answer:\n", answer)
